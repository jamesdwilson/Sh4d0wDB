#!/usr/bin/env bash
# =============================================================================
# import-md — Import Agent .md Files into ShadowDB
# =============================================================================
#
# This script reads Markdown files from an agent workspace directory and
# imports them into a ShadowDB database with opinionated categorization.
#
# WHAT IT DOES:
#   Reads each .md file, determines where it belongs in the database based on
#   the filename, and inserts it. Different files go to different places:
#
#   SOUL.md, IDENTITY.md   → startup table (injected every session as identity)
#   USER.md                → memories table (category: personal)
#   MEMORY.md              → memories table (category: general)
#   BOOTSTRAP.md           → memories table (category: ops)
#   TOOLS.md               → SKIPPED (managed by the agent framework)
#   AGENTS.md              → SKIPPED (will be replaced with "DB: m query")
#   HEARTBEAT.md           → SKIPPED (managed by the agent framework)
#   *.md                   → memories table (auto-categorized by filename pattern)
#
# WHY THE STARTUP TABLE IS SPECIAL:
#   The startup table holds the agent's identity — who it is, who its owner is,
#   what rules it follows. This is returned by `m` on the first query of each
#   session, so the model knows who it is before getting search results.
#   SOUL.md and IDENTITY.md go here because they define the agent's character.
#
# WHY SOME FILES ARE SKIPPED:
#   TOOLS.md and AGENTS.md are framework-managed files. The agent framework
#   reads them directly — they shouldn't be duplicated in the database.
#   After import, AGENTS.md will be overwritten with `DB: m query` anyway.
#
# SAFETY:
#   This script READS files — it does NOT delete or modify them.
#   Your original .md files are untouched. Always back up first anyway.
#   (quickstart.sh does this automatically.)
#
# USAGE:
#   ./import-md <workspace-dir>                        # Import with defaults
#   ./import-md <workspace-dir> --dry-run              # Preview without writing
#   ./import-md <workspace-dir> --backend sqlite       # Use SQLite instead
#   ./import-md --help                                 # Show help
#
# EXAMPLES:
#   ./import-md ~/.openclaw/workspace/                 # Standard OpenClaw workspace
#   ./import-md ~/my-agent/ --backend sqlite --dry-run # Preview SQLite import
#
# SEE ALSO:
#   import-md-optimized — Minified version (same functionality, fewer comments)
#   quickstart.sh       — Automated setup that calls this script
#   m / m-universal     — Search CLI that queries the imported data

# Exit immediately on error, undefined variable, or pipe failure.
# This is defensive — we don't want partial imports on failure.
set -euo pipefail

# =============================================================================
# CONFIGURATION VARIABLES
# =============================================================================

# Path to the ShadowDB JSON config file.
# Can be overridden with the SHADOWDB_CONFIG environment variable.
# This file contains backend connection details (psql path, database name, etc.)
CONFIG_FILE="${SHADOWDB_CONFIG:-$HOME/.shadowdb.json}"

# Command-line flags (set by argument parsing below)
DRY_RUN=false       # If true, show what would happen without writing to DB
WORKSPACE=""         # Path to the workspace directory containing .md files
BACKEND=""           # Database backend: "postgres" or "sqlite"

# Counters for the summary at the end
IMPORTED=0
SKIPPED=0

# =============================================================================
# USAGE / HELP
# =============================================================================

usage() {
  echo "Usage: $0 <workspace-dir> [--dry-run] [--backend postgres|sqlite]"
  echo ""
  echo "Import agent .md files into ShadowDB."
  echo ""
  echo "Options:"
  echo "  --dry-run     Show what would be imported without writing to the database"
  echo "  --backend     Override backend (default: read from ~/.shadowdb.json)"
  echo "  --help, -h    Show this help message"
  echo ""
  echo "File mapping:"
  echo "  SOUL.md, IDENTITY.md   → startup table (agent identity)"
  echo "  USER.md                → memories (category: personal)"
  echo "  MEMORY.md              → memories (category: general)"
  echo "  BOOTSTRAP.md           → memories (category: ops)"
  echo "  TOOLS.md, AGENTS.md    → skipped (framework-managed)"
  echo "  *.md                   → memories (auto-categorized)"
  exit 1
}

# =============================================================================
# ARGUMENT PARSING
# =============================================================================
# We parse args manually (no getopt) to keep the script portable across
# macOS and Linux. The structure is simple enough that a while loop works.

while [[ $# -gt 0 ]]; do
  case "$1" in
    --dry-run)
      DRY_RUN=true
      shift
      ;;
    --backend)
      # --backend requires a value argument
      BACKEND="$2"
      shift 2
      ;;
    --help|-h)
      usage
      ;;
    *)
      # First non-flag argument is the workspace directory.
      # Any additional non-flag arguments are an error.
      if [[ -z "$WORKSPACE" ]]; then
        WORKSPACE="$1"
        shift
      else
        echo "Error: unexpected argument '$1'" >&2
        usage
      fi
      ;;
  esac
done

# Workspace directory is required
[[ -z "$WORKSPACE" ]] && usage

# Strip trailing slash for consistent path joining later
WORKSPACE="${WORKSPACE%/}"

# Validate the workspace directory exists
if [[ ! -d "$WORKSPACE" ]]; then
  echo "Error: '$WORKSPACE' is not a directory" >&2
  exit 1
fi

# =============================================================================
# BACKEND DETECTION
# =============================================================================
# If --backend wasn't specified, read it from the config file.
# Default to "postgres" if no config exists (it's the recommended backend).
#
# We use a one-liner Python snippet to parse the JSON config because:
#   - bash has no built-in JSON parser
#   - jq is a common dependency we'd rather not require
#   - python3 is already required for the `m` scripts
# The `2>/dev/null || echo "postgres"` ensures we get a sane default
# if the config file is missing, malformed, or python3 isn't available.

if [[ -z "$BACKEND" ]] && [[ -f "$CONFIG_FILE" ]]; then
  BACKEND=$(python3 -c "import json; print(json.load(open('$CONFIG_FILE')).get('backend','postgres'))" 2>/dev/null || echo "postgres")
fi
BACKEND="${BACKEND:-postgres}"

# =============================================================================
# DATABASE HELPER FUNCTIONS
# =============================================================================
# Each backend gets its own set of helper functions:
#   db_exec()         — Run arbitrary SQL
#   insert_startup()  — Insert/update a startup table row
#   insert_memory()   — Insert a new memory record
#
# WHY DOLLAR-QUOTED STRINGS ($md$...$md$)?
#   PostgreSQL's dollar-quoting lets us safely insert content containing
#   single quotes, double quotes, backslashes, and other special characters
#   without escaping. The delimiter ($md$) is arbitrary — we use "md" for
#   "markdown". The content between $md$ and $md$ is treated as a literal
#   string. This is critical because .md files often contain code blocks,
#   SQL examples, and other content with special characters.
#
# WHY ON CONFLICT DO UPDATE?
#   The startup table has a PRIMARY KEY on the `key` column. If you re-run
#   import-md, it updates the existing rows instead of failing. This makes
#   the script idempotent — you can run it as many times as you want.

if [[ "$BACKEND" == "postgres" ]]; then

  # Read PostgreSQL connection details from config
  DB_NAME=$(python3 -c "import json; print(json.load(open('$CONFIG_FILE')).get('postgres',{}).get('database','shadow'))" 2>/dev/null || echo "shadow")
  PSQL=$(python3 -c "import json; print(json.load(open('$CONFIG_FILE')).get('postgres',{}).get('psql_path','psql'))" 2>/dev/null || echo "psql")

  # Execute raw SQL against the database.
  # -q suppresses command-processing output (notices, etc.)
  db_exec() { "$PSQL" -q "$DB_NAME" -c "$1"; }

  # Insert or update a row in the startup table.
  # The startup table holds identity/soul/rules — one row per key.
  insert_startup() {
    local key="$1" content="$2"
    if $DRY_RUN; then
      echo "  [DRY RUN] INSERT startup: key=$key (${#content} chars)"
      return
    fi
    # ON CONFLICT (key) DO UPDATE makes this idempotent —
    # re-running import-md updates existing identity instead of failing.
    db_exec "INSERT INTO startup (key, content) VALUES ('$key', \$md\$$content\$md\$)
             ON CONFLICT (key) DO UPDATE SET content = EXCLUDED.content;"
    echo "  ✓ startup.$key (${#content} chars)"
  }

  # Insert a new memory record.
  # Memories are the searchable knowledge base — contacts, cases, knowledge, etc.
  # Tags are stored as a PostgreSQL text array for efficient filtering.
  insert_memory() {
    local title="$1" content="$2" category="$3" tags="$4"
    if $DRY_RUN; then
      echo "  [DRY RUN] INSERT memory: $title → $category (${#content} chars)"
      return
    fi
    # Dollar-quoted strings prevent SQL injection from .md content.
    # Tags are wrapped in {} for PostgreSQL array syntax: '{tag1,tag2,tag3}'.
    db_exec "INSERT INTO memories (title, content, category, tags)
             VALUES (\$t\$$title\$t\$, \$c\$$content\$c\$, '$category', '{$tags}');"
    echo "  ✓ $title → $category (${#content} chars)"
  }

elif [[ "$BACKEND" == "sqlite" ]]; then

  # Read SQLite database path from config
  DB_PATH=$(python3 -c "import json; print(json.load(open('$CONFIG_FILE')).get('sqlite',{}).get('db_path','$HOME/.shadowdb/shadow.db'))" 2>/dev/null || echo "$HOME/.shadowdb/shadow.db")

  db_exec() { sqlite3 "$DB_PATH" "$1"; }

  insert_startup() {
    local key="$1" content="$2"
    if $DRY_RUN; then
      echo "  [DRY RUN] INSERT startup: key=$key (${#content} chars)"
      return
    fi
    # INSERT OR REPLACE is SQLite's equivalent of PostgreSQL's ON CONFLICT DO UPDATE
    sqlite3 "$DB_PATH" "INSERT OR REPLACE INTO startup (key, content) VALUES ('$key', '$content');"
    echo "  ✓ startup.$key (${#content} chars)"
  }

  insert_memory() {
    local title="$1" content="$2" category="$3" tags="$4"
    if $DRY_RUN; then
      echo "  [DRY RUN] INSERT memory: $title → $category (${#content} chars)"
      return
    fi
    sqlite3 "$DB_PATH" "INSERT INTO memories (title, content, category, tags) VALUES ('$title', '$content', '$category', '$tags');"
    echo "  ✓ $title → $category (${#content} chars)"
  }

else
  echo "Error: unsupported backend '$BACKEND'" >&2
  echo "Supported backends: postgres, sqlite" >&2
  exit 1
fi

# =============================================================================
# FILE → CATEGORY MAPPING
# =============================================================================
# This function determines where each .md file should go in the database.
# The return value is a colon-separated string with the format:
#   target:details
#
# Where target is one of:
#   "startup"  — goes to the startup table (key is in details)
#   "memory"   — goes to the memories table (category:tag is in details)
#   "skip"     — don't import this file (reason is in details)
#
# DESIGN DECISION: Opinionated by default
#   We map known agent framework filenames to specific categories because
#   we know what they contain. SOUL.md is always the agent's personality.
#   USER.md is always the owner's preferences. This is opinionated but
#   correct for the common case. Unknown files get auto-categorized by
#   filename patterns (keywords like "research", "fitness", "finance").
#
# WHY NOT AUTO-CATEGORIZE EVERYTHING?
#   We could use embeddings or LLM classification to categorize unknown
#   files. But that would add latency, require Ollama to be running during
#   import, and introduce non-determinism. Filename-based heuristics are
#   fast, deterministic, and surprisingly accurate for most agent workspaces.

file_category() {
  local basename="$1"
  case "$basename" in
    # === STARTUP TABLE (agent identity) ===
    # These define WHO the agent is. They're loaded once per session
    # via the startup table, not searched — they're always-present context.
    SOUL.md|SOUL.txt)         echo "startup:soul" ;;
    IDENTITY.md|IDENTITY.txt) echo "startup:identity" ;;

    # === MEMORIES TABLE (searchable knowledge) ===
    # These contain content the agent should be able to SEARCH for.
    # They're not injected every turn — only returned when relevant.
    USER.md|USER.txt)         echo "memory:personal:user-profile" ;;
    MEMORY.md|MEMORY.txt)     echo "memory:general:memory-index" ;;
    BOOTSTRAP.md)             echo "memory:ops:bootstrap" ;;

    # === SKIPPED FILES ===
    # These are managed by the agent framework and shouldn't be in the DB.
    # TOOLS.md: Framework reads this directly for tool guidance.
    # AGENTS.md: Will be replaced with "DB: m query" — no need to import.
    # HEARTBEAT.md: Framework reads this for periodic task config.
    TOOLS.md)                 echo "skip:framework-managed" ;;
    AGENTS.md)                echo "skip:will-be-replaced" ;;
    HEARTBEAT.md)             echo "skip:will-be-replaced" ;;

    # === AUTO-CATEGORIZATION BY FILENAME PATTERNS ===
    # For files we don't recognize, we guess the category from keywords
    # in the filename. This catches common naming conventions.
    *research*)               echo "memory:research:$basename" ;;
    *fitness*|*health*)       echo "memory:fitness:$basename" ;;
    *finance*|*tax*|*budget*) echo "memory:finance:$basename" ;;
    *project*|*plan*)         echo "memory:project:$basename" ;;
    *meeting*|*prep*)         echo "memory:meeting-prep:$basename" ;;
    *contact*|*people*)       echo "memory:contacts:$basename" ;;
    *decision*|*choice*)      echo "memory:decision:$basename" ;;

    # === FALLBACK ===
    # Everything else goes into the "general" category.
    # Users can recategorize later with SQL: UPDATE memories SET category = ...
    *)                        echo "memory:general:$basename" ;;
  esac
}

# =============================================================================
# MAIN IMPORT LOOP
# =============================================================================

echo "═══════════════════════════════════════"
echo " ShadowDB Import — $BACKEND"
echo " Source: $WORKSPACE"
$DRY_RUN && echo " MODE: DRY RUN (no writes)"
echo "═══════════════════════════════════════"
echo ""

# Find all .md files up to 2 levels deep (workspace root + one subdirectory).
# We limit depth to avoid descending into node_modules, .git, etc.
# Results are sorted for deterministic output order.
find "$WORKSPACE" -maxdepth 2 -name "*.md" -type f | sort | while read -r filepath; do
  filename=$(basename "$filepath")

  # Relative path for display (strip workspace prefix)
  relpath="${filepath#$WORKSPACE/}"

  # Read the file content
  content=$(cat "$filepath")

  # Skip empty or trivially small files (< 5 chars after stripping spaces).
  # Empty SOUL.md is common in fresh workspaces — no point importing it.
  if [[ -z "${content// /}" ]] || [[ ${#content} -lt 5 ]]; then
    echo "  ⊘ $relpath (empty/trivial — skipped)"
    SKIPPED=$((SKIPPED + 1))
    continue
  fi

  # Determine where this file goes (startup table, memories table, or skip)
  mapping=$(file_category "$filename")

  # Parse the colon-separated mapping string.
  # Example: "memory:personal:user-profile"
  #   target = "memory"
  #   rest   = "personal:user-profile"
  target="${mapping%%:*}"    # Everything before the first colon
  rest="${mapping#*:}"       # Everything after the first colon

  case "$target" in
    startup)
      # Startup table: rest is the key (e.g., "soul", "identity")
      key="$rest"
      echo "→ $relpath → startup.$key"
      insert_startup "$key" "$content"
      IMPORTED=$((IMPORTED + 1))
      ;;
    memory)
      # Memories table: rest is "category:tag"
      category="${rest%%:*}"   # e.g., "personal"
      tag="${rest#*:}"         # e.g., "user-profile"
      echo "→ $relpath → memories ($category)"
      insert_memory "$filename" "$content" "$category" "imported,md-import,$tag"
      IMPORTED=$((IMPORTED + 1))
      ;;
    skip)
      # Don't import — just log why
      reason="$rest"
      echo "  ⊘ $relpath ($reason)"
      SKIPPED=$((SKIPPED + 1))
      ;;
  esac
done

# =============================================================================
# SUMMARY AND NEXT STEPS
# =============================================================================

echo ""
echo "═══════════════════════════════════════"
echo " Done."
echo ""
echo " Next steps:"
echo "   1. Verify: m \"test query\""
echo "   2. Zero originals: for f in SOUL.md USER.md MEMORY.md; do echo -n > \"\$f\"; done"
echo "   3. Set AGENTS.md: echo 'DB: m query' > AGENTS.md"
echo "   4. Set HEARTBEAT.md: echo 'm d' > HEARTBEAT.md"
echo "═══════════════════════════════════════"
